try:
    import time, sys
    from typing import Union, Dict, List
    from statistics import mean

    from beartype import beartype
    from typeguard import typechecked
    import type_enforced

    from pydantic import BaseModel, validate_call

    # Open the log file, clear it and redirect stdout to it
    log = open("benchmark.md", "w")
    sys.stdout.flush()  # Ensure the log file is cleared before writing
    sys.stdout = log

    REPEATS = 100

    # --- Test data
    five_key_dict = {f'key{i}': i for i in range(5)}
    big_key_dict = {f'key{i}': i for i in range(1000)}

    five_item_list = [1, 2.0, 3, 4.0, 5]
    big_item_list = [float(i) if i % 2 else i for i in range(1000)]

    # --- Benchmark and Validation test cases
    test_cases = {
        "int": (42, "not an int"),
        "Union[int,float]": (3.14, "not a number"),
        "str": ("hello", 123),
        "dict[str,int] (5 keys)": (five_key_dict, {"k1": 1, "k2": "two", "k3": 3}),
        "dict[str,int] (1000 keys)": (big_key_dict, {"k1": 1, "k2": "two", "k3": 3}),
        "list[Union[int,float]] (5 items)": (five_item_list, [1, "two", 3, 4, 5]),
        "list[Union[int,float]] (1000 items)": (big_item_list, [1, "two", 3, 4, 5]*200),
        "list[dict[str,int]] (5 items)": ([five_key_dict] * 5, [{"k1": 1, "k2": "two", "k3": 3}]),
        "list[dict[str,int]] (100 items)": ([big_key_dict] * 100, [{"k1": 1, "k2": "two", "k3": 3}]),
    }

    # --- Typing definitions
    types = {
        "int": int,
        "Union[int,float]": Union[int, float],
        "str": str,
        "dict[str,int] (5 keys)": Dict[str, int],
        "dict[str,int] (1000 keys)": Dict[str, int],
        "list[Union[int,float]] (5 items)": List[Union[int, float]],
        "list[Union[int,float]] (1000 items)": List[Union[int, float]],
        "list[dict[str,int]] (5 items)": List[Dict[str, int]],
        "list[dict[str,int]] (100 items)": List[Dict[str, int]],
    }

    # --- Timing helper
    def timeit(func, arg):
        durations = []
        for _ in range(REPEATS):
            start = time.perf_counter()
            func(arg)
            durations.append(time.perf_counter() - start)
        return mean(durations) * 1e6  # microseconds

    # --- Factory functions
    def pydantic_factory(typ):
        class PModel(BaseModel):
            x: typ

        @validate_call
        def f(x: typ) -> None:
            PModel(x=x)
        return f

    def beartype_factory(typ):
        @beartype
        def f(x: typ) -> None:
            pass
        return f

    def typeguard_factory(typ):
        @typechecked
        def f(x: typ) -> None:
            pass
        return f

    def type_enforced_factory(typ):
        @type_enforced.Enforcer()
        def f(x: typ) -> None:
            pass
        return f

    # --- Checkers and factories
    checkers = {
        "type_enforced": type_enforced_factory,
        "Pydantic": pydantic_factory,
        "Beartype": beartype_factory,
        "Typeguard": typeguard_factory,
    }

    # --- Validation helper
    def test_validation(func, valid_value, invalid_value):
        try:
            func(valid_value)
            valid_passed = True
        except Exception:
            valid_passed = False

        try:
            func(invalid_value)
            invalid_passed = False
        except Exception:
            invalid_passed = True

        return valid_passed and invalid_passed

    # --- Final output
    print(f"# Benchmark Results (python {sys.version.split(' ')[0]})\n")
    print("This file contains the results of the benchmark tests for various type checkers.\n")
    print("Generated by /test/benchmark.py\n")
    print("Each checker is tested with different data types and structures")
    print("- Every checker gets the same data and test cases")
    print(f"- The average time taken to validate the data over {REPEATS} runs is measured.")
    print("\n## Results Summary")
    print("The following table summarizes the average time taken by each type checker for different data types and structures.\n")
    print("- Note: N/A indicates that the validation failed for the given type or structure.")
    print("    - This could be due to the type checker not raising an error when it should or raising an error when it shouldn't.")
    print(f"- Note: It is also worth noting that Beartype (0.21.0 at initial writing) inconsistently catches type errors in nested structures (including with the same data).")
    print(f"    - The validation is run {REPEATS} times to ensure type checking results are consistent.")
    print("\n| Type                        | type_enforced  | Pydantic       | Beartype       | Typeguard     |")
    print("|:-----------------------------|:----------------|:----------------|:----------------|:----------------|")


    def green_text(text):
        return f"<span style='color: green;'>{text}</span>"

    def red_text(text):
        return f"<span style='color: red;'>{text}</span>"
    
    data = {}

    for case, (valid_val, invalid_val) in test_cases.items():
        typ = types[case]
        case_data = {}
        for name, factory in checkers.items():
            try:
                fn = factory(typ)
                avg_us = timeit(fn, valid_val)
                passed = all([test_validation(fn, valid_val, invalid_val) for _ in range(REPEATS)])
                avg_us_colored = green_text(f"{avg_us:.2f} Âµs") if passed else red_text(f"N/A")
                case_data[name] = avg_us_colored
            except Exception as e:
                case_data[name] = red_text(f"Error")
        data[case] = case_data
    for case, results in data.items():
        print(f"| {case:<30} | " + " | ".join(results.values()) + " |")
    sys.stdout = sys.__stdout__  # Reset stdout to original
    log.close()  # Close the log file
    print("benchmark.py passed")
except Exception as e:
    sys.stdout = sys.__stdout__
    try:
        log.close()
    except:
        pass
    print(f"benchmark.py failed: {e}")